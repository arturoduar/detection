{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageClothingPropertyClassifier_TFHub_basic_tester.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1JofWf7qtmZiA-EkjV2ZJWXa7-76bf1CP",
      "authorship_tag": "ABX9TyNUuXzbtE2LwHrrtpqtBLaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arturoduar/detection/blob/main/testers/ImageClothingPropertyClassifier_TFHub_basic_tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7U9a_0wy965"
      },
      "source": [
        "# **Main Skeleton Tester - *Tensoflow Hub* Image Detector Pattern**\n",
        "\n",
        "This script illustrates the basic skeleton of the extraction of an item's feature (color) from an image that contains the item. The basic logic is:\n",
        "\n",
        "For each image: \n",
        "- Detect where in the image the item is located by producing a detection box (sub-image) that contains the best representation of the item.\n",
        "- Crop the image to isolate the item, then process the sub-image with feature ckassifiers. In this first attempt, we try to estract the best color that matches the item. \n",
        "\n",
        "Script Parts Sequence:\n",
        "\n",
        "1. Imports\n",
        "2. Load detectors / classifiers / add restrictions\n",
        "  - In this TF Hub-based script, we can use Inception_Resnet_V2, or Mobilenet V2 (lesser accuracy, but faster). Both are trained from the Openimages V4 dataset. The detection restriction we add is to only keep results from the 'Clothing' branch of the dataset. \n",
        "  - The color classifier is a K-Means model from SciKit Learn, that is then matched via minimum RGB distance to our own list of available detailed colors.    \n",
        "3. Load data file\n",
        "  - This is a file with the same format as the one used to submit data to SAP Upscale, with columns for 'asin' and the ssociated image URLs\n",
        "4. Process a chosen image:\n",
        "\n",
        "  a. import image from URL\n",
        "\n",
        "  b. reshape image to match detector input\n",
        "\n",
        "  c. detect from the image possible item boxes\n",
        "\n",
        "  d. use a criteria for selecting the box that best captures the item we want to examine\n",
        "  - In this tester, we use the highest scored item from a list that is common to all of them (intersection)\n",
        "\n",
        "  e. crop the image to isolate the item.\n",
        "\n",
        "  f. analyze the sub-image with the classifies (color in this case)\n",
        "  - The color is picked from the highest scored color among all images \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GFTZSIKPJm-"
      },
      "source": [
        "!pip install webcolors  # may not be really needed in the short future"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RpWjl7RywLB"
      },
      "source": [
        "# 1. imports\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "# For running inference on the TF-Hub module for the inception model \n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# main working imports\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "#import gzip\n",
        "import subprocess\n",
        "import numpy as np\n",
        "#from datetime import datetime\n",
        "import requests\n",
        "#import bs4\n",
        "\n",
        "# imnporting, displaying, and drawing into images imports\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# For drawing onto the image.\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "import cv2 as cv\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "import webcolors \n",
        "import os\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2sA_vjAqNrW"
      },
      "source": [
        "# 2.a\n",
        "# load image detection module - models from Tensorflow Hub:\n",
        "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\" \n",
        "# module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\"\n",
        "\n",
        "# TF Hub uses the pattern below to evaluate pre-processed images \n",
        "# i.e. given a valid input image 'my_image', run the detector simply like this:\n",
        "#   result = detector('my_image')\n",
        "detector = hub.load(module_handle).signatures['default']\n",
        "\n",
        "# define the classes that are admissible (in this case, under the main class 'Clothing' from OpenInages V4)\n",
        "class_inclusion_set = set([\"/m/07qxg_\", \"/m/03p3bw\", \"/m/080hkjn\", \"/m/0584n8\", \"/m/01s55n\", \n",
        "                           \"/m/01940j\", \"/m/0zvk5\", \"/m/0hf58v5\", \"/m/0h8mhzd\", \"/m/03nfch\",\n",
        "                           \"/m/06k2mb\", \"/m/01b638\", \"/m/02p3w7d\", \"/m/09j5n\", \"/m/0fly7\",\n",
        "                           \"/m/07mhn\", \"/m/04tn4x\", \"/m/0nl46\", \"/m/0jyfg\", \"/m/0hnnb\", \"/m/0gjkl\",\n",
        "                           \"/m/080hkjn\", \"/m/02h19r\", \"/m/02wbtzl\", \"/m/02jfl0\", \"/m/02fq_6\", \"/m/025rp__\",\n",
        "                           \"/m/02dl1y\", \"/m/02_n6y\", \"/m/01rkbr\", \"/m/01r546\", \"/m/01nq26\", \"/m/01llwg\",\n",
        "                           \"/m/01krhy\", \"/m/017ftj\", \"/m/0176mf\", \"/m/03grzl\", \"/m/0174n1\", \"/m/0463sg\",\n",
        "                           \"/m/032b3c\", \"/m/01cmb2\", \"/m/02wv6h6\", \"/m/02h19r\", \"/m/02wbtzl\", \"/m/02jfl0\",\n",
        "                           \"/m/02fq_6\", \"/m/025rp__\", \"/m/02dl1y\", \"/m/01xyhv\", \"/m/01xygc\", \"/m/01n4qj\",\n",
        "                           \"/m/01krhy\", \"/m/01gmv2\", \"/m/01gkx_\", \"/m/01d40f\", \"/m/01bfm9\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jbR0X3ptItn"
      },
      "source": [
        "# 2.b\n",
        "# Color detection using K-Means \n",
        "\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "\n",
        "HEX_TO_NORMALIZED_NAMES = {\n",
        "    \"#f5f5dc\": \"beige\",\n",
        "    \"#000000\": \"black\",\n",
        "    \"#0000ff\": \"blue\",\n",
        "    \"#a52a2a\": \"brown\",\n",
        "    \"#808080\": \"gray\",\n",
        "    \"#808080\": \"grey\",\n",
        "    \"#008000\": \"green\",\n",
        "    \"#ffa500\": \"orange\",\n",
        "    \"#ffc0cb\": \"pink\",\n",
        "    \"#800080\": \"purple\",\n",
        "    \"#ff0000\": \"red\",\n",
        "    \"#ffffff\": \"white\",\n",
        "    \"#ffff00\": \"yellow\",\n",
        "}\n",
        "\n",
        "def closest_color(requested_color):\n",
        "    min_colors = {}\n",
        "    #for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
        "    for key, name in HEX_TO_NORMALIZED_NAMES.items():  \n",
        "      r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
        "      rd = (r_c - requested_color[0]) ** 2\n",
        "      gd = (g_c - requested_color[1]) ** 2\n",
        "      bd = (b_c - requested_color[2]) ** 2\n",
        "      min_colors[(rd + gd + bd)] = name\n",
        "    return min_colors[min(min_colors.keys())]\n",
        "\n",
        "def get_color_name(requested_color):\n",
        "    try:\n",
        "      closest_name = webcolors.rgb_to_name(requested_color)\n",
        "    except ValueError:\n",
        "      closest_name = closest_color(requested_color)\n",
        "    return closest_name\n",
        "\n",
        "def detect_color(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  height, width, dim = img.shape\n",
        "  img = img[int(height/4):int(3*height/4), int(width/4):int(3*width/4), :]\n",
        "  height, width, dim = img.shape\n",
        "    \n",
        "  img_vec = np.reshape(img, [height * width, dim] )\n",
        "    \n",
        "  #kmeans = KMeans(n_clusters=4)\n",
        "  kmeans.fit( img_vec )\n",
        "\n",
        "  unique_l, counts_l = np.unique(kmeans.labels_, return_counts=True)\n",
        "  sort_ix = np.argsort(counts_l)\n",
        "  sort_ix = sort_ix[::-1]\n",
        "\n",
        "  cluster_center = kmeans.cluster_centers_[sort_ix][0]\n",
        "\n",
        "  # return cluster_center\n",
        "  \n",
        "  return get_color_name((int(cluster_center[2]), int(cluster_center[1]), int(cluster_center[0])))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7uKKAShzMcO"
      },
      "source": [
        "# 3\n",
        "# Read the data to be examined (csv)\n",
        "df_img_data = pd.read_csv(\"ImagesToExamine.csv\") \n",
        "\n",
        "df_img_data.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGDWjLssQZnj"
      },
      "source": [
        "# add a column to collect analysis\n",
        "df_img_data[\"img_feature_analysis\"] = \"\"\n",
        "#list(df_img_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWkyS-nHUFPA"
      },
      "source": [
        "# Main detection helper functions\n",
        "def display_image(image):\n",
        "  fig = plt.figure(figsize=(20, 15))\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image)\n",
        "\n",
        "def download_and_resize_image(url, new_width=256, new_height=256,\n",
        "                              display=False):\n",
        "  _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "  response = urlopen(url)\n",
        "  image_data = response.read()\n",
        "  image_data = BytesIO(image_data)\n",
        "  pil_image = Image.open(image_data)\n",
        "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "  pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "  pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "  print(\"Image downloaded to %s.\" % filename)\n",
        "  if display:\n",
        "    display_image(pil_image)\n",
        "  return filename\n",
        "\n",
        "def draw_bounding_box_on_image(image,\n",
        "                               ymin,\n",
        "                               xmin,\n",
        "                               ymax,\n",
        "                               xmax,\n",
        "                               color,\n",
        "                               font,\n",
        "                               thickness=4,\n",
        "                               display_str_list=()):\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)],\n",
        "            width=thickness,\n",
        "            fill=color)\n",
        "\n",
        "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "\n",
        "  if top > total_display_str_height:\n",
        "    text_bottom = top\n",
        "  else:\n",
        "    text_bottom = top + total_display_str_height\n",
        "  # Reverse list and print from bottom to top.\n",
        "  for display_str in display_str_list[::-1]:\n",
        "    text_width, text_height = font.getsize(display_str)\n",
        "    margin = np.ceil(0.05 * text_height)\n",
        "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "    draw.text((left + margin, text_bottom - text_height - margin),\n",
        "              display_str,\n",
        "              fill=\"black\",\n",
        "              font=font)\n",
        "    text_bottom -= text_height - 2 * margin\n",
        "\n",
        "def draw_box(winner, path, image, boxes, class_names, scores, max_boxes=1, min_score=0.5):\n",
        "  colors = list(ImageColor.colormap.values())\n",
        "\n",
        "  try:\n",
        "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "  except IOError:\n",
        "    print(\"Font not found, using default font.\")\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "  print('winner = ' + winner)\n",
        "\n",
        "  lidx = np.where(class_names == winner.encode(\"ascii\"))\n",
        "  nidx = lidx[0]\n",
        "  idx = nidx[np.argmax(scores[nidx])] \n",
        "  #print('idx = ' + str(idx))\n",
        "  #print(class_names[idx].decode(\"ascii\"))\n",
        "  score = scores[idx]\n",
        "  ymin, xmin, ymax, xmax = tuple(boxes[idx])\n",
        "  display_str = \"{}: {}%\".format(class_names[idx].decode(\"ascii\"),\n",
        "                                int(100 * scores[idx]))\n",
        "  color = colors[hash(class_names[idx]) % len(colors)]\n",
        "  image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "  draw_bounding_box_on_image(\n",
        "          image_pil,\n",
        "          ymin,\n",
        "          xmin,\n",
        "          ymax,\n",
        "          xmax,\n",
        "          color,\n",
        "          font,\n",
        "          display_str_list=[display_str])\n",
        "  np.copyto(image, np.array(image_pil))\n",
        "  im_width, im_height = image.shape[:2]\n",
        "  #print(\"im: \" +  str(im_width) + ',      ' + str(im_height))\n",
        "  cropped, croped_path = crop_image(path, xmin, ymin, xmax, ymax, im_width, im_height)\n",
        "\n",
        "  im_width, im_height = image.shape[:2]\n",
        "\n",
        "  return image, cropped, display_str, score\n",
        "\n",
        "def crop_image(path, xmin, ymin, xmax, ymax, im_width, im_height):\n",
        "    myImage = cv.imread(str(path))\n",
        "    # print(\"path: \" + str(path))\n",
        "    # print(\"cords: \" + str(xmin) + ', ' + str(ymin) + ', '  + str(xmax) + ', '   + str(ymax))\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width, \n",
        "                                      ymin * im_height, ymax *im_height)\n",
        "    myImage = cv.resize(myImage,(im_width,im_height))\n",
        "    cropped = myImage[int(top): int(bottom), int(left):int(right)]\n",
        "    cv.imwrite(\"img_cropped.jpg\", cropped)\n",
        "    croped_path = \"img_cropped.jpg\"\n",
        "    # print(croped_path)\n",
        "    return cropped, croped_path"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZhCQz523CEb"
      },
      "source": [
        "# 4\n",
        "# Process Images\n",
        "\n",
        "main_results = {}\n",
        "\n",
        "# main function to add features to a row of input data\n",
        "def add_img_features(in_params):\n",
        "  out_params = in_params \n",
        "\n",
        "  # 4.a Make a list of image urls\n",
        "  image_urls = in_params[6].split('|')\n",
        "  print(image_urls)\n",
        "\n",
        "  img_feature_analysis = {}\n",
        "  patterns = set()\n",
        "  colors = set()\n",
        "\n",
        "  for image_url in image_urls:\n",
        "    print(image_url)\n",
        "    image_path = download_and_resize_image(image_url, 640, 640)\n",
        "    plt.imshow(load_img(image_path))\n",
        "    plt.show()\n",
        "\n",
        "    print(image_path)\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # 4.b Pre-process image\n",
        "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "    #start_time = time.time()\n",
        "    # 4.c Use the Object Detector on processed image\n",
        "    result = detector(converted_img)\n",
        "    #end_time = time.time()\n",
        "\n",
        "    result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "    main_results[image_url] = result\n",
        "\n",
        "  # form a set with all the common, relevant images\n",
        "  detect_intersection = set([])\n",
        "  for image in main_results:\n",
        "    #print(image)\n",
        "    cleaned_list = []\n",
        "    for ithclass in main_results[image]['detection_class_names']:\n",
        "      if ithclass.decode(\"ascii\").lower() in class_inclusion_set: \n",
        "        cleaned_list.append(True)\n",
        "      else:\n",
        "        cleaned_list.append(False)  \n",
        "\n",
        "    running_set = set(main_results[image]['detection_class_entities'][cleaned_list])\n",
        "    if len(detect_intersection) == 0:\n",
        "      detect_intersection = running_set\n",
        "    else:\n",
        "      detect_intersection = detect_intersection.intersection(running_set)  \n",
        "\n",
        "    # print(main_results[image]['detection_class_entities'][cleaned_list])\n",
        "    # print(set(main_results[image]['detection_class_entities'][cleaned_list]))  \n",
        "  print(detect_intersection)\n",
        "\n",
        "  # 4.d\n",
        "  # Get the scores for the common items\n",
        "  item_scores = {}\n",
        "  item_winner = {}\n",
        "  for item in detect_intersection:\n",
        "    scores = []\n",
        "    for image in main_results:\n",
        "      idx = np.where(main_results[image]['detection_class_entities'] == item)\n",
        "      # print(idx)\n",
        "      scores = np.concatenate((scores, main_results[image]['detection_scores'][idx]), axis=0)\n",
        "      # scores.append(main_results[image]['detection_scores'][idx]) \n",
        "\n",
        "    avg_score = np.average(scores)\n",
        "    item_scores[item.decode(\"ascii\")] = avg_score\n",
        "    if item_winner == {}:\n",
        "      item_winner[item.decode(\"ascii\")] = avg_score\n",
        "      item_winner_label = item.decode(\"ascii\")\n",
        "    else:\n",
        "      if avg_score > list(item_winner.values())[0]:\n",
        "        item_winner = {}\n",
        "        item_winner[item.decode(\"ascii\")] = avg_score \n",
        "        item_winner_label = item.decode(\"ascii\")\n",
        "\n",
        "  image_colors = {}\n",
        "  for image_url in image_urls:\n",
        "    print(image_url)\n",
        "    # image_path = download_and_resize_image(image_url, 640, 480)\n",
        "    image_path = download_and_resize_image(image_url, 640, 640)\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "    #print(\"Found %d objects.\" % len(main_results[image_url][\"detection_scores\"]))\n",
        "    #print(\"Inference time: \", end_time-start_time)\n",
        "    #print(result)\n",
        "\n",
        "    # 4.e\n",
        "    # Crop the image \n",
        "    image_with_boxes, cropped_img, display_str, winner_score = draw_box(item_winner_label, image_path,\n",
        "      img.numpy(), main_results[image_url][\"detection_boxes\"],\n",
        "      main_results[image_url][\"detection_class_entities\"], main_results[image_url][\"detection_scores\"])\n",
        "    \n",
        "    plt.imshow(image_with_boxes)\n",
        "    plt.show()\n",
        "\n",
        "    print('cropped. Class: ' + display_str)\n",
        "    plt.imshow(cropped_img)\n",
        "    plt.show()\n",
        "    #cropped_img = load_img(\"img_cropped.jpg\", target_size = (64, 64))\n",
        "    cropped_img = img_to_array(cropped_img)\n",
        "    cropped_img = np.expand_dims(cropped_img, axis = 0)\n",
        "\n",
        "    # 4.f\n",
        "    # Run the cropped image through the particular feature classifiers (in this \n",
        "    # script there is only one classifier - color)\n",
        "    prediction_color = detect_color(\"img_cropped.jpg\")\n",
        "    colors.add(str(prediction_color))\n",
        "    image_colors[winner_score] = str(prediction_color)\n",
        "\n",
        "    print('predicted color: ' + str(prediction_color))\n",
        "\n",
        "  print(image_colors)\n",
        "  main_color = image_colors[min(image_colors.keys())]\n",
        "\n",
        "  if len(colors) != 0:\n",
        "    #img_feature_analysis['colors'] = list(colors)  \n",
        "    img_feature_analysis['colors'] = main_color\n",
        "\n",
        "  out_json = json.dumps(img_feature_analysis)\n",
        "  out_params[17] = out_json\n",
        "\n",
        "  return out_params\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skCQqHPlRipt"
      },
      "source": [
        "sample = df_img_data.loc[df_img_data['asin'] == 'B00N1SZXSM'].values.flatten().tolist()\n",
        "# sample = df_img_data.loc[df_img_data['asin'] == 'B00HV713ME'].values.flatten().tolist()\n",
        "# sample = df_img_data.loc[df_img_data['asin'] == 'B00DR4918S'].values.flatten().tolist()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoUKlidl7rIk"
      },
      "source": [
        "sample[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgRrYOO3RzU9"
      },
      "source": [
        "sampleresult = add_img_features(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL3wWek8SFnM"
      },
      "source": [
        "sampleresult[17]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIIWU2DtyjKn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}